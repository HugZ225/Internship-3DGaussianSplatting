# CHANGES 

Here you can find the modifications and additions I made to the original GitHub repository.  
I made these modifications during my internship at the GTI lab, ETSIT-UPM Madrid, from April to August 2025.  
In this document, I provide explanations of several methods, accompanied by examples and insights drawn from my experience.

## Training with a PLY 

To train a dataset, we need the COLMAP files: cameras.bin, images.bin, and points3D.bin.  
I modified the *colmap_loader.py* and *dataset_readers.py* files to enable training with a points3D.ply file instead of a .bin file (I kept a backup of the original versions).

## Pointcloud 

I made two functions to generate point clouds (.ply files) from the images and depth maps of the dataset:  
*Pointcloud_from1Cam.py*: generates a point cloud from a single camera point of view.  
*FVVpointcloud.py*: generates a point cloud by combining information from all the cameras.

## Useful Tools

The COLMAP files need to have a special format, for example the type of the cameras has to be PINHOLE, and I made function to convert others types of cameras (SIMPLE_PINHOLE, OPEN_CV) into pinhole. 

All this functions are in the Useful Tools folder 

## Visualization 

The remote viewer and real-time visualization didnâ€™t work on my computer.

To visualize the results, I first used Blender with  this GitHub repository (https://github.com/Kiri-Innovation/3dgs-render-blender-addon/blob/main/README.md) and followed this tutorial (https://www.youtube.com/watch?v=WUL73wQDtcE&t=634s).

Later, someone recommended this web viewer (https://antimatter15.com/splat/) to quickly preview the splats with better performance.

## Iterations 

In the training script, I modified the test_iterations and save_iterations to include more frequent checkpoints.
Instead of only testing at iterations 7000 and 30000, the following values are now used:
300, 1000, 3000, 5000, 7000, 10000, 15000, 22000, 30000
This allows for a better visualization of the training progress over time.

## Depths 

To generate the depth map, I put a redirection to Depth-Anything-V2

## Dataset 

I trained the algorithms on several datasets.  

- Playroom :
This dataset from INRIA represents a playroom that can be explored from a 360-degree perspective. It was the first one I used to start experimenting with the algorithms.
The results were very good and I also tried the depth parameter with this dataset.

- Arco Valentino :
This dataset was provided to me by GTI. It contains images captured while rotating around the Arch of Turin.
I was given COLMAP files and a point cloud. I used the convert algorithm to have the converted images and trained the model using the provided COLMAP data, with different point clouds (the one provided by convert algorithm, one generated by me, and the one provided by the GTI lab).

- First FVV:
This dataset shows a person from GTI walking in a room recorded by 4 cameras. Each camera contains 255 frames, but for training, we only used the first frame from each camera.
I also had access to the corresponding depth images.
I used a function to generate a point cloud from the 4 camera views, and then I trained the model.
The results with the generated point cloud were very good.

- Monkey Dataset:
This dataset was also provided by GTI and includes COLMAP files. It consists of 9 camera views.
I followed the same procedure as before, but the results with the generated point cloud were not satisfactory.
The best results I obtained were with the converted point cloud.


